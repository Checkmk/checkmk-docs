// -*- coding: utf-8 -*-
// IGNORE %
include::global_attr.adoc[]
= Monitoring cluster services
:title: Monitoring cluster services
:description: You can set up certain services to be provided by changing hosts within a cluster. Learn here how to configure the monitoring for such clusters in {CMK}.

{related-start}
xref:hosts_setup#[Host administration]
xref:agent_windows#[Monitoring Windows]
{related-end}


[#intro]
== Introduction

[#cluster]
=== Clusters, nodes and cluster services

When deploying critical and mission-critical services such as databases or _E-Commerce_ web sites, you will be unlikely to be relying on the host running those services to live a long, stable, rash-free life.
Rather, you will factor in the possible failure of one host and ensure that other hosts are on standby to take over services immediately in the event of a failure (_or failover_),
so that the failure will not even be noticeable to the outside world.

A group of networked hosts working together to accomplish the same task is called a computer network or computer cluster, or more simply, a cluster.
A cluster acts and appears as a single system externally and organizes its hosts internally to work together to accomplish the common task.

A cluster can perform various tasks, for example an HPC cluster can perform _high-performance computing_,
which is used, among other scenarios, when calculations require much more memory than is available on a single computer.
If the cluster has the task of providing _high availability_, it is also called an HA cluster.
This article is concerned with HA clusters, i.e. when we refer to a 'cluster' in the following text, we always mean an *HA cluster*.

A cluster offers one or more services to the outside world: the cluster services, sometimes referred to as 'clustered services'.
In a cluster, the hosts that make it up are called _nodes_.
At any given time, each service is provided by just one of the nodes.
If any node in the cluster fails, all services essential to the cluster's mission are moved to one of the other nodes.

To make any failover transparent, some clusters provide their own cluster IP address, which is sometimes also referred to as a virtual IP address.
The cluster IP address always refers to the active node and is representative of the entire cluster.
In the event of a failover, the IP address is transferred to another, previously passive node, which then becomes the active node.
The client that communicates with the cluster can be oblivious to an internal failover:
it uses the same, unchanged IP address and does not itself need to do any switching.

Other clusters do not have a cluster IP address.
Oracle database clusters in many of their variants are a prominent example.
Without a cluster IP address, the client must maintain a list of IP addresses of all nodes that could provide the service.
If the active node fails, the client must detect this and switch to the node that is now providing the service.


[#monitoring]
=== Monitoring a cluster

{CMK} is one of the clients that communicates with the cluster.
In {CMK}, all of the nodes in a cluster can be set up and monitored
-- regardless of how the cluster software internally checks the status of the individual nodes and, if necessary, performs a failover.

Most of the checks that {CMK} performs on the individual nodes of a cluster deal with the physical properties of the nodes,
which are independent of whether the host belongs to a cluster or not.
Examples include CPU and memory usage, local disks, physical network interfaces, etc.
However, to map the cluster function of the nodes in {CMK}, it is necessary to identify those services that define the cluster's task and a transfer to another node might be necessary
-- the cluster services.

{CMK} helps you to monitor the cluster services. What you need to do is:

. Create the cluster.
. Select the cluster services.
. Perform a service discovery for all of the associated hosts.

How to proceed is described in the next chapter using the following sample configuration:

[{image-border}]
image::cs_example_cluster.png[]

In {CMK}, a Windows failover cluster is to be set up as an HA cluster consisting of two nodes with Microsoft SQL (MS SQL) Servers installed.
This is a so-called active/passive cluster, which means that only one, the active node, runs a database instance.
The other node is passive and only becomes active in the event of a failover, when it will boot the database instance and replace the failed node.
The data in the database instance is not stored on the nodes themselves, but on a shared storage medium,
e.g. a storage area network (SAN), to which both nodes are connected.
The sample configuration consists of the following components:

* `mssql-node01` is the active node running an active database instance.
* `mssql-node02` is the passive node.
* `mssql-cluster01` is the cluster to which both nodes belong.

In contrast to this example, it is also possible that the same node can be included in more than one cluster.
In the last chapter you will learn how to configure such overlapping clusters using a modified sample configuration.


[#setup]
== Setting up clusters and cluster services

[#create]
=== Creating a cluster

In {CMK}, the nodes and the cluster itself are created as hosts (node hosts and cluster hosts), with a special host type defined for a cluster host.

Here are some points to consider before setting up a cluster host:

* The cluster host is a virtual host to be configured with a cluster IP address if one is present.
In our example, we assume that the cluster host name is resolvable via DNS.

* Cluster hosts can be configured in the same way as 'normal' hosts, for example with xref:glossar#host_tag[host tags] or xref:glossar#host_group[host groups].

* For all participating hosts (this always means the cluster host and all its associated node hosts), the data sources must be configured identically,
i.e. in particular, some may not be configured via a {CMK} agent and others via 
ifdef::onprem[]
SNMP.
endif::[]
ifdef::saas[]
xref:glossar#special_agent[special agent].
endif::[]
{CMK} ensures that a cluster host can only be created if this requirement is met.

ifdef::onprem[]
* In a xref:distributed_monitoring#[distributed monitoring] all participating hosts must be assigned to the same {CMK} site.
endif::[]

* Not all checks work in a cluster configuration.
For those checks that have cluster support implemented, you can read about this in the manual page of the plug-in.
You can access the manual pages from the menu [.guihint]#Setup > Services > Catalog of check plug-ins#.

In our example, the two node hosts `mssql-node01` and `mssql-node02` have already been created and set up as xref:glossar#host[hosts].
To find out how to get this far, see the xref:agent_windows#[article on monitoring Windows servers]
-- and there in the chapter on extending the standard Windows agent with plug-ins, for our example the link:https://checkmk.com/integrations?tags=mssql[MS SQL Server plug-ins^].

Start the creation of the cluster from the menu [.guihint]#Setup > Hosts > Hosts#  and then from the menu [.guihint]#Hosts > Add cluster#:

image::cs_create_cluster.png[]

Enter `mssql-cluster01` as the [.guihint]#Host name#, and enter the two node hosts under [.guihint]#Nodes#.

[TIP]
====
If you are dealing with a cluster without a cluster IP address, you will need to take a not-so-comfortable detour,
by selecting [.guihint]#No IP# in the [.guihint]#Network address# box for the [.guihint]#IP address family#.
But to prevent the host from going {DOWN} in the monitoring, you must change the default 'Host check command' for this via the rule of the same name 
-- from [.guihint]#Smart PING# or [.guihint]#PING# to, for example, the state of one of the services which is to be assigned to the cluster host -- as will be explained in the next section.
For more information on host rule sets, see the xref:wato_rules#[article on rules].
====

Complete the creation with [.guihint]#Save & view folder# and xref:wato#activate_changes[activate the changes].


[#select_services]
=== Selecting cluster services

{CMK} cannot know which of the services running on a node are local and which are cluster services
-- some file systems may be local, others may be mounted only on the active node.
The same is true for processes: While the 'Windows Timer' service is most likely running on all nodes, a particular database instance will only be available on the active node.

Instead of making {CMK} guess, select the cluster services with a rule.
Without a rule, no services will be assigned to the cluster.
We will assume in this example that the names of all MS SQL Server cluster services begin with `MSSQL` and that the file system in the shared storage device is accessible via the `D:` drive

Start with [.guihint]#Setup > Hosts > Hosts# and click the cluster name.
On the [.guihint]#Properties of host# page select from the menu [.guihint]#Host > Clustered services#.
You will land on the [.guihint]#Clustered services# rule set page where you can create a new rule.
You will then receive the [.guihint]#Add rule: Clustered services# page:

image::cs_rule_cs.png[]

Regardless of whether and how the hosts are organized into folders,
be sure to create any rules for cluster services so that they apply to the node hosts on which the services run.
Such a rule is ineffective for a cluster host.

In the [.guihint]#Conditions# box, under [.guihint]#Folder#, select the folder that contains the node hosts.
Enable [.guihint]#Explicit hosts# and enter the active node host `mssql-node01` and the passive node host `mssql-node02`.
Then enable [.guihint]#Services# and make two entries there:
`MSSQL` for all MS SQL services whose name starts with `MSSQL` and `Filesystem D:` for the drive.
The entries are interpreted as xref:regexes#[regular expressions].

All services that are not defined as cluster services will be treated as local services by {CMK}.

Finish creating the rule with [.guihint]#Save# and activate the changes.


[#discovery]
=== Perform a service discovery

For all participating hosts (cluster and node hosts), a new xref:glossar#service_discovery[service discovery] must be performed at the end
so that all newly defined cluster services are first removed from the nodes and then added to the cluster.

Under [.guihint]#Setup > Hosts > Hosts#, first select all hosts involved and then select from the menu [.guihint]#Hosts > On Selected hosts > Run bulk service discovery#.
On the [.guihint]#Bulk discovery# page, the first option [.guihint]#Add unmonitored services and new host labels# should produce the desired result.

Click [.guihint]#Start# to begin the xref:wato_services#bulk_discovery[service discovery for multiple hosts].
Upon successful completion -- indicated by the `Bulk discovery successful` message -- exit and activate the changes.

To find out whether the selection of cluster services has led to the desired result, you can list all services that are now assigned to the cluster:
Under [.guihint]#Setup > Hosts > Hosts#, in the host list at the cluster host entry click the icon:icon_services[] icon to edit the services.
On the following page [.guihint]#Services of host# all cluster services are listed under [.guihint]#Monitored services#:

image::cs_cluster_monitored_services.png[]

On the other hand, for node hosts, those very services which have been moved to the cluster will now be missing from the list of monitored services.
On the node host, you can find these again by looking at the end of the services list in the [.guihint]#Monitored clustered services (located on cluster host)# section:

image::cs_node_monitored_services.png[]

[TIP]
====
If you run xref:localchecks#[local checks] in a cluster where the [.guihint]#Clustered services# rule has been applied,
you can use the [.guihint]#Local checks in {CMK} clusters# rule set to influence the result by choosing between [.guihint]#Worst state# and [.guihint]#Best state#.
====


[#auto_discovery]
=== Automatic service discovery

If you let the service discovery be done automatically via the xref:wato_services#discovery_auto[Discovery Check], you have to consider a special aspect.
The [.guihint]#Discovery Check# can automatically delete disappearing services.
However, if a clustered service moves from one node to another, it could be incorrectly registered as vanished and then deleted.
On the other hand, if you omit this option, services that actually disappeared would never be deleted.


[#overlapping]
== Overlapping clusters

It is possible for several clusters to share one or more nodes.
These are then referred to as overlapping clusters.
For overlapping clusters, you need a special rule to tell {CMK} which cluster services of a shared node host should be assigned to which cluster.

Below we will present the basic procedure for setting up an overlapping cluster 
by modifying the example of the MS SQL Server cluster from an active/passive to an active/active cluster:

[{image-border}]
image::cs_example_cluster_overlap.png[]

In this configuration, not only is MS SQL Server installed on both node hosts, but a separate database instance is running on each of the two nodes.
Both nodes access the shared storage medium, but on different drives.
This example implements a 100{nbsp}% overlapping cluster because the two nodes belong to both clusters.

The advantage with the active/active cluster is that the available resources of the two nodes are better utilized.
In the event of a failover, the task of the failed node is taken over by the alternative node, which then runs both database instances.

This sample configuration thus consists of the following components:

* `mssql-node01` is the first active node currently running the database instance `MSSQL Instance1`.
* `mssql-node02` is the second active node currently running the database instance `MSSQL Instance2`.
* `mssql-cluster01` and `mssql-cluster02` are the two clusters to which both nodes belong.

You only need to slightly modify the first step for setting up the active/passive cluster for an active/active cluster:
You create the first cluster `mssql-cluster01` as described above.
Then you create the second cluster `mssql-cluster02` with the same two node hosts.

In the second step, instead of using the general [.guihint]#Clustered services# rule set to select cluster services, use the rule set especially-created for overlapping clusters [.guihint]#Clustered services for overlapping clusters#.
This allows you to define in a rule the cluster services that will be removed from the node hosts and added to the selected cluster.

For our example with 100{nbsp}% overlap we need two of these rules:
The first rule defines the cluster services of the first database instance, which run on the first node host by default.
Since in the event of a failover these cluster services are transferred to the second node host, we assign the services to both node hosts.
The second rule does the same for the second cluster and the second database instance.

Let's start with the first rule:
Under [.guihint]#Setup > General > Rule search# find the rule set [.guihint]#Clustered services for overlapping clusters# and click on it.
Create a new rule.
Under [.guihint]#Assign services to the following cluster# enter the cluster `mssql-cluster01`:

image::cs_rule_cs_overlapping.png[]

In the [.guihint]#Conditions# box, under [.guihint]#Folder# again select the folder that contains the node hosts.
Enable [.guihint]#Explicit hosts# and enter both node hosts.
Next, activate [.guihint]#Services# and make two entries there:
`MSSQL Instance1` for all MS SQL services from the first database instance, and `Filesystem D:` for the drive:

image::cs_rule_cs_overlapping_conditions.png[]

Finish the creation of the first rule with [.guihint]#Save#.

Then create the second rule, this time for the second cluster `mssql-cluster02` and again for both node hosts.
Under [.guihint]#Services# you now enter `MSSQL Instance2` for all MS SQL services in the second database instance.
The second node host, on which the second database instance runs by default, accesses its storage medium under a different drive, in the following example via the `E:` drive:

image::cs_rule_cs_overlapping_conditions2.png[]

Save this rule as well and then activate the two changes.

Finally, perform a service discovery as the third and very last step in the same way as described above -- as a [.guihint]#Bulk discovery# for all of the associated hosts, i.e. the two cluster hosts and the two node hosts.

[TIP]
====
If multiple rules define a cluster service, the more specific rule [.guihint]#Clustered services for overlapping clusters# with the explicit assignment to a specific cluster takes precedence over the more general rule [.guihint]#Clustered services#.
For the two examples presented in this article, this means that the last two specific rules created would never allow the general rule created in the first example to apply.
====
