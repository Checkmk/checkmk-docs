include::global_attr.adoc[]
= Monitoring Kubernetes
:revdate: draft
:title: Monitoring Kubernetes
:description: With {CMK} you can also monitor the container orchestration software Kubernetes. Read details about the agentless facility here.

{related-start}
xref:wato_monitoringagents#[Monitoring agents]
xref:monitoring_docker#[Monitoring Docker]
link:https://checkmk.com/integrations[Catalog of Check Plug-ins^]
{related-end}


== Foreword

There may still be changes to both this description and the new Kubernetes monitoring feature in {CMK} version {v21} itself.
Please see the link:https://github.com/tribe29/checkmk-docs/commits/master/de/monitoring_kubernetes.asciidoc[Changes to this article on GitHub^] or our link:https://checkmk.com/de/werks?search=kubernetes&product=cmk&cmk_version%5B%5D=2.1[Werks regarding changes to the feature^] itself.

Since Kubernetes integration with {CMK} is natively built into Kubernetes itself, we also directly use the _README_ files in the GitHub repositories.
In particular, the link:https://github.com/tribe29/checkmk_kube_agent/tree/main/deploy/charts/checkmk[Instructions for installing the agent^] is a direct source to read up on the current recommended ways.


=== Getting started with the Kubernetes monitoring

For an introduction to the new monitoring of Kubernetes, we recommend the two videos link:https://www.youtube.com/watch?v=H9AlO98afUE[Kubernetes Monitoring with {CMK}^] and link:https://www.youtube.com/watch?v=2H-cLhyfYbc[Detecting issues and configuring alerts for Kubernetes clusters^].


=== Differences to the previous Kubernetes monitoring

Kubernetes monitoring in {CMK} has been rewritten from scratch. The amount of data that can be monitored has grown significantly. Since the technical basis for Kubernetes monitoring is fundamentally different in {CMK} {v21}, it is not possible to adopt or even rewrite previous monitoring data for your Kubernetes objects.


== Introduction

Kubernetes has been the most widely used tool for container orchestration for quite some time. {CMK} helps you monitor your Kubernetes environments.
Starting with version {v21}, you can use {CMK} to monitor the following Kubernetes objects:

* Cluster
* Nodes
* Deployments
* Pods
* DaemonSets
* StatefulSets

For a complete listing of all available check plugins for Kubernetes monitoring, see our link:https://checkmk.com/integrations?tags=kubernetes[Catalog of Check Plug-ins^].


== Setting up the monitoring

=== Perequisites in the cluster

==== Setting up via Helm Chart

Currently, we recommend installing Kubernetes monitoring using the `helm` tool, as it is suitable for less experienced users. To do this, first of all, make the repository known. In the following example, we use the name `k8scmk` to make it easier to access the repository later. You can of course use any other name:

[{shell}]
----
{c-user} helm repo add k8scmk \https://tribe29.github.io/checkmk_kube_agent
----

After updating the freshly cloned repository with `helm repo update`, you can do the rest of the installation with the second step. Here you assign a so-called _namespace_ -- in the example this is 'checkmk-monitoring' and a _release name_. If you set the latter to `checkmk`, you automatically benefit from giving Kubernetes objects short names. Lastly, specify the path where `helm` will find the necessary configuration files:

[{shell}]
----
{c-user} helm upgrade --install --create-namespace -n checkmk-monitoring checkmk k8scmk/checkmk
----

Alternatively, you can specify at the end of the command if you want to use a specific version. For example, you could specify `-- version 1.0.0-beta.2` here, or specific markers, such as the `--devel` in the following example:

[{shell}]
----
{c-user} helm upgrade --install --create-namespace -n checkmk-monitoring checkmk k8scmk/checkmk --devel
----

More information can be found in the _README_ file of the link:https://github.com/tribe29/checkmk_kube_agent/tree/main/deploy/charts/checkmk[Helm Charts^] already mentioned in the introduction.


==== Setting up via manifest

If you already have a lot of experience with Kubernetes and want to have control over all details of the setup, setting up Kubernetes monitoring with the manifests we prefabricated is also suitable. To do this, first download the manifests we have prebuilt from our corresponding link:https://github.com/tribe29/checkmk_kube_agent/tree/main/deploy/kubernetes[repository at GitHub^].
You will need at least the following five files:

 00_namespace.yaml
 checkmk-serviceaccount.yaml
 cluster-collector.yaml
 node-collector.yaml
 service.yaml

We have split the entire configuration into several files to facilitate their maintenance or to provide more concise files for clearly defined purposes.
Specifically, these manifests serve the following purposes:

[cols="55,~"]
|===
|filename |purpose
|`00_namespace.yaml` |Create namespace named checkmk-monitoring
|`checkmk-serviceaccount.yaml` |Create service account named checkmk and cluster role named checkmk-metrics-reader in namespace checkmk-monitoring
|`cluster-collector.yaml` |Here we create the cluster collector we have named. Among other things, a service account named cluster-collector is created in the namespace checkmk-monitoring and the service accounts are assigned roles within the cluster. In addition, the deployment named cluster-collector is defined.
|`node-collector.yaml` |Analogous to `cluster-collector.yaml` for the nodes
|`service.yaml` | Create service named cluster-collector in namespace checkmk-monitoring. Create a service named cluster-collector-nodeport in the namespace checkmk-monitoring. The port for the NodePort is also specified here.
|===

If you don't want to clone the whole repo right away - which you are free to do, of course - you can use the following command to download specifically the five files you need:

[{shell}]
----
{c-user} URL='https://raw.githubusercontent.com/tribe29/checkmk_kube_agent/main/deploy/kubernetes/'; for i in 00_namespace checkmk-serviceaccount cluster-collector node-collector service; do wget "${URL}${i}.yaml"; done
----

If you also want to set up a network policy and a pod security policy, you also need the following two files:

 network-policy.yaml
 pod-security-policy.yaml

[{shell}]
----
{c-user} URL='https://raw.githubusercontent.com/tribe29/checkmk_kube_agent/main/deploy/kubernetes/'; for i in network-policy pod-security-policy; do wget "${URL}${i}.yaml"; done
----

In the files `cluster-collector.yaml` and `node-collector.yaml` you have to fill four placeholders with concrete content.
In both files you will find places where `main_<YYY.MM.DD>` is written.
Replace these placeholders with tags from our link:https://hub.docker.com/r/checkmk/kubernetes-collector/tags[Kubernetes collector on Docker Hub^].
For example, you could use the following command to replace all occurrences of `main_<YYY.MM.DD>` with the March 1, 2022 build tag of our container.

[{shell}]
----
{c-user} sed -i 's/main_<YYYY.MM.DD>/main_2022.03.01/g' node-collector.yaml cluster-collector.yaml
----

For the communication to the outside a service of the type NodePort is needed.
This allows communication from the outside and is also permanently set to TCP port 30035 in the `service.yaml` file.
If this port is already assigned in your cluster, please change the port accordingly.

Once you have made these settings, you can apply these manifest files collectively to your cluster.
To do this, run the following command from the manifest location:

[{shell}]
----
{c-user} kubectl apply -f .
namespace/checkmk-monitoring created
serviceaccount/checkmk created
clusterrole.rbac.authorization.k8s.io/checkmk-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/checkmk-metrics-reader-binding created
serviceaccount/cluster-collector created
clusterrolebinding.rbac.authorization.k8s.io/checkmk-cluster-collector created
clusterrolebinding.rbac.authorization.k8s.io/checkmk-token-review created
deployment.apps/cluster-collector created
serviceaccount/node-collector-machine-sections created
serviceaccount/node-collector-container-metrics created
clusterrole.rbac.authorization.k8s.io/node-collector-container-metrics-clusterrole created
podsecuritypolicy.policy/node-collector-container-metrics-podsecuritypolicy created
clusterrolebinding.rbac.authorization.k8s.io/node-collector-container-metrics-cluterrolebinding created
daemonset.apps/node-collector-container-metrics created
daemonset.apps/node-collector-machine-sections created
service/cluster-collector created
service/cluster-collector-nodeport created
----

You can also use `kubectl` to check whether the manifests have been applied correctly.
To do this, use the following command to display all the pods in the `checkmk-monitoring` namespace:

[{shell}]
----
{c-user} kubectl get pods -n checkmk-monitoring
----

Furthermore, you can also check all services within the namespace as follows:

[{shell}]
----
{c-user} kubectl get svc -n checkmk-monitoring
----


=== Prerequisites in {CMK}

Next, in the GUI of {CMK}, we move on to setting up the xref:glossar#special_agent[specialagent] and a rule to automatically create hosts for your Kubernetes objects.
However, to set up the special agent, there are a few prerequisites that need to be met first:


[#token]
==== Store password (token) in {CMK}

The best way to store the password (token) of the service account is to store it in the password store of {CMK}.
This is the most secure variant, because you can separate the storage and use of the password organizationally.
Alternatively, enter it directly in plain text when creating the rule (see below).

If you have kept the default `checkmk-monitoring` as the namespace for monitoring your Kuberentes cluster, the following command line will cut the password directly from the output of `kubectl get secrets`:

[{shell}]
----
{c-user} kubectl get secret $(kubectl get serviceaccount checkmk -o=jsonpath='{.secrets[*].name}' -n checkmk-monitoring) -n checkmk-monitoring -o=jsonpath='{.data.token}' | base64 --decode
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJjaGVjay1tayIsI
mt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjaGVjay1tay10b2tlbi16OWhicCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5
hbWUiOiJjaGVjay1tayIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjIxODE3OWEzLTFlZTctMTFlOS1iZjQzLTA4MDAyN2E1ZjE0MSIsInN1YiI6I
nN5c3RlbTpzZXJ2aWNlYWNjb3VudDpjaGVjay1tazpjaGVjay1tayJ9.gcLEH8jjUloTeaAj-U_kRAmRVIiETTk89ujViriGtllnv2iKF12p0L9ybT1fO-1Vx7XyU8jneQRO9lZw8JbhVmaPjrkEc8
kAcUdpGERUHmVFG-yj3KhOwMMUSyfg6wAeBLvj-y1-_pMJEVkVbylYCP6xoLh_rpf75JkAicZTDmhkBNOtSf9ZMjxEmL6kzNYvPwz76szLJUg_ZC636OA2Z47qREUtdNVLyutls7ZVLzuluS2rnfoP
JEVp_hN3PXTRei0F5rNeA01wmgWtDfo0xALZ-GfvEQ-O6GjNwHDlsqYmgtz5rC23cWLAf6MtETfyeEJjRqwituhqUJ9Jp7ZHgQ%
----

The password is really that long.
If you work directly under Linux, you can add a `| xsel --clipboard` at the end.
Then the password is not printed at all, but copied directly to the clipboard (as if you had copied it with the mouse):

[{shell}]
----
{c-user} kubectl get secret $(kubectl get serviceaccount checkmk -o=jsonpath='{.secrets[*].name}' -n checkmk-monitoring) -n checkmk-monitoring -o=jsonpath='{.data.token}' | base64 --decode | xsel --clipboard
----

Add the password to the {CMK} password store with [.guihint]#Setup > General > Passwords > Add password# e.g. under the ID and the title `kubernetes`:

image::kubernetes_password.png[]


[#certimport]
==== Import CA of the service account into {CMK}

In order for {CMK} to trust the Certificate Authority (CA) of the service account, you must store the CA certificate in {CMK}. You can read out the certificate - provided you have kept `checkmk-monitoring` as the namespace - with the following command:

[{shell}]
----
{c-user} kubectl get secret $(kubectl get serviceaccount checkmk -o=jsonpath='{.secrets[*].name}' -n checkmk-monitoring) -n checkmk-monitoring -o=jsonpath='{.data.ca\.crt}' | base64 --decode
----

Copy everything here including the lines `BEGIN CERTIFICATE` and `END CERTIFIACTE` and add the certificate in the Setup menu under [.guihint]#Setup > General > Global settings > Site management > Trusted certificate authorities for SSL#.

image::kubernetes_ca.png[]

[#source-host]
==== Create Piggyback source host

Create a new host in Checkmk in the usual way and name it `mykubernetesclusterhost`, for example.
As the title and host name suggest, this host is used to collect the Piggyback data and also to map all services and metrics at the cluster level.
Since this host only receives data via the special agent, set the [.guihint]#IP address family# option to [.guihint]#No IP#.

==== Set up dynamic host configuration

To ensure separation between the numerous Kubernetes objects and the rest of your monitoring environment, it is a good idea to first create a folder via [.guihint]#Setup > Hosts > Add folder# in which the xref:dcd#[dynamic host configuration] can automatically create all necessary hosts.
Creating or using such a folder is certainly to be considered optional.

However, it is absolutely necessary to set up a connector for the piggyback data. Via [.guihint]#Setup > Hosts > Dynamic host management > Add connection# you get to the page for the corresponding setup.
First enter a title and then click [.guihint]#show more# under [.guihint]#Connection Properties#.

Next, click [.guihint]#Add new element# and under [.guihint]#Create hosts in# select the folder you created earlier.

In a Kubernetes environment, where monitorable and monitored objects naturally come and go, it is also recommended to enable the [.guihint]#Automatically delete hosts without piggyback data# option.
What exactly this option does and under what circumstances hosts are then actually deleted is explained in the section xref:dcd#_automatically_delete_hosts[Automatically deleting hosts] in the article on dynamic host configuration.

Now enter the previously created xref:source-host[Piggyback source host] under [.guihint]#Restrict source hosts# and enable the [.guihint]#Discover services during creation# option.

The [.guihint]#Connection Properties# section of this new connector might look like the following afterwards:

image::monitoring_kubernetes_connection_properties.png[alt="Sample dynamic host configuration settings."]

[#rule]
=== Setting up the special agent

Now that all the prerequisites are in place in the cluster and in {CMK}, you can turn your attention to configuring the special agent.
This can be found via [.guihint]#Setup > Agents > VM, Cloud, Container > Kubernetes#.

First of all, you need to assign a name for the cluster you want to monitor.
You can choose this name freely.
It is used to give a unique name to all objects that originate from exactly this cluster.
For example, if you enter `mycluster` here, the names of the hosts of all pods from this cluster will later start with `pod_mycluster`.
The next part of the host name will then always be the namespace in which this Kubernetes object exists.

Under [.guihint]#Token#, now select the xref:token[previously created entry] from the password store of {CMK}.

Under [.guihint]#API server connection > Endpoint# {CMK} now asks you to enter the URL (or IP address) through which your Kubernetes API server can be reached.
The port is also required here.
The easiest way to find out this IP address - if you don't already have it at hand - depends on your Kubernetes environment.
For a Vanilla Kubernetes, `kubectl` is again a good place to start.
You can use the following command to view all endpoints in your environment.

[{shell}]
----
{c-user} kubectl get endpoints --all-namespaces
----

Depending on the context `kubectl` is currently set to, you can omit `--all-namespaces` if necessary.
If you have set the CA of your cluster - xref:certimport[as described above] - in {CMK}, you can select under [.guihint]#SSL certificate verification# the [.guihint]#Verify the certificate# option.

If your Kubernetes API server is only accessible via a proxy or if special timeouts are required for the connection, you can enter them under [.guihint]#HTTP proxy# and [.guihint]#TCP timeouts#.

Next, you have the choice to enrich the monitoring of your Kubernetes cluster with usage data collected by the {CMK} cluster collector.
To do this, you need to specify the protocol, URL, and port of the cluster collector under [.guihint]#Collector NodePort/Ingress endpoint#.
If you set it up using our manifests, the port here is `30035` by default.
If you have customized the port in the `service.yaml` file, please change the port here accordingly.
You should be able to find out the URL or IP address of the nodeport from the description of the `cluster-collector` pod.
Just run the following command and look in the line that starts with `Node`:

[{shell}]
----
{c-user} kubectl describe pod $(kubectl get pods --no-headers -o custom-columns=":metadata.name") | grep -A5 Name:.*cluster-collector
Name:         cluster-collector-5b7c8468cf-5t5hj
Namespace:    checkmk-monitoring
Priority:     0
Node:         minikube/[green]#172.16.23.2#
Start Time:   Wed, 03 Mar 2022 20:54:45 +0100
Labels:       app=cluster-collector
----

With the options [.guihint]#Collect information about...# you can now finally select which objects within your cluster should be monitored.
Our preselection covers the most relevant objects.
If you decide to monitor the [.guihint]#Pods of CronJobs# as well, please refer to the xref:user_interface#inline_help[inline help] for this point.

Last but not least, you can choose whether you want to monitor only certain namespaces within your clusters or whether explicit namespaces should be excluded from monitoring.
You specify this using the [.guihint]#Monitor namespaces# option.

Your rule might now look like the following:

image::monitoring_kubernetes_rule.png[alt="Exemplary completed rule for Kubernetes special agent."]

*Important:* Under [.guithint]#Conditions > Explicit hosts# you must now re-enter the xref:source-host[previously created host]:

image::monitoring_kubernetes_explicit_hosts.png[alt="Rules for special agents must always be set to explicit hosts, as seen here."]

Next, store the rule and perform a service discovery on this host.
You will see the first cluster-level services right here:

image::monitoring_kubernetes_service_discovery.png[alt="Exemplary view of the first service discovery after the configuration is complete."]

Afterwards, activate all the changes you made and let the dynamic host configuration do the work from now on.
It will generate all hosts for your Kubernetes objects after a short time.

== Labels for Kubernetes objects

{CMK} automatically generates labels for Kubernetes objects such as clusters, deployments, or namespaces during service discovery.
All labels to Kubernetes objects that {CMK} automatically generates start with `cmk/kubernetes/`.
For example, a pod always gets a label of the node (`cmk/kubernetes/node:mynode`), a label that just shows that this object is a pod (`cmk/kubernetes/object:pod`) and a label for the namespace (`cmk/kubernetes/namespace:mynamespace`).
This makes it very easy to create filters and rules for all objects of the same type or in the same namespace.

== Hardware/Software Inventory

Kubernetes monitoring of {CMK} also supports xref:inventory#[HW/SW inventory].

image::kubernetes_monitoring_hw_sw_inventory.png[alt="Exemplary view of hardware and software inventory of a pod."]

== Removing {CMK}

If you have deployed {CMK} to your cluster via our manifests, you can remove created accounts, service and so on just as easily as they were set up.
To do this, go back to the directory where the YAML files are located and run the following command:

[{shell}]
----
{c-user} kubectl delete -f .
namespace "checkmk-monitoring" deleted
serviceaccount "checkmk" deleted
clusterrole.rbac.authorization.k8s.io "checkmk-metrics-reader" deleted
clusterrolebinding.rbac.authorization.k8s.io "checkmk-metrics-reader-binding" deleted
serviceaccount "cluster-collector" deleted
clusterrolebinding.rbac.authorization.k8s.io "checkmk-cluster-collector" deleted
clusterrolebinding.rbac.authorization.k8s.io "checkmk-token-review" deleted
deployment.apps "cluster-collector" deleted
serviceaccount "node-collector-machine-sections" deleted
serviceaccount "node-collector-container-metrics" deleted
clusterrole.rbac.authorization.k8s.io "node-collector-container-metrics-clusterrole" deleted
podsecuritypolicy.policy "node-collector-container-metrics-podsecuritypolicy" deleted
clusterrolebinding.rbac.authorization.k8s.io "node-collector-container-metrics-cluterrolebinding" deleted
daemonset.apps "node-collector-container-metrics" deleted
daemonset.apps "node-collector-machine-sections" deleted
service "cluster-collector" deleted
service "cluster-collector-nodeport" deleted
----
