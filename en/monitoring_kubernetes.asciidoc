include::global_attr.adoc[]
= Monitoring Kubernetes
:revdate: draft
:title: Monitoring Kubernetes
:description: With {CMK} you can also monitor the container orchestration software Kubernetes. Read details about the agentless facility here.

{related-start}
xref:wato_monitoringagents#[Monitoring agents]
xref:monitoring_docker#[Monitoring Docker]
link:https://checkmk.com/integrations[Catalog of Check Plug-ins^]
{related-end}


== Foreword

There may still be changes to both this description and the new Kubernetes monitoring feature in {CMK} version {v21} itself.
Please see the link:https://github.com/tribe29/checkmk-docs/commits/master/de/monitoring_kubernetes.asciidoc[Changes to this article on GitHub^] or our link:https://checkmk.com/de/werks?search=kubernetes&product=cmk&cmk_version%5B%5D=2.1[Werks regarding changes to the feature^] itself.

Since Kubernetes integration with {CMK} is natively built into Kubernetes itself, we also directly use the _README_ files in the GitHub repositories.
In particular, the link:https://github.com/tribe29/checkmk_kube_agent/tree/main/deploy/charts/checkmk[Instructions for installing the agent^] is a direct source to read up on the current recommended ways.


=== Getting started with the Kubernetes monitoring

For an introduction to the new monitoring of Kubernetes, we recommend the two videos link:https://www.youtube.com/watch?v=H9AlO98afUE[Kubernetes Monitoring with {CMK}^] and link:https://www.youtube.com/watch?v=2H-cLhyfYbc[Detecting issues and configuring alerts for Kubernetes clusters^].


=== Differences to the previous Kubernetes monitoring

Kubernetes monitoring in {CMK} has been rewritten from scratch. The amount of data that can be monitored has grown significantly. Since the technical basis for Kubernetes monitoring is fundamentally different in {CMK} {v21}, it is not possible to adopt or even rewrite previous monitoring data for your Kubernetes objects.


== Introduction

Kubernetes has been the most widely used tool for container orchestration for quite some time. {CMK} helps you monitor your Kubernetes environments.

Starting with version {v21}, you can use {CMK} to monitor the following Kubernetes objects:

* Cluster
* Nodes
* Deployments
* Pods
* DaemonSets
* StatefulSets

For a complete listing of all available check plugins for Kubernetes monitoring, see our link:https://checkmk.com/integrations?tags=kubernetes[Catalog of Check Plug-ins^].


== Perequisites in the cluster

To be able to monitor your Kubernetes cluster in {CMK},
first create the prerequisites in your cluster.
First and foremost,
tell the cluster which pods/containers to deploy and how to configure them.

=== Setting up the Helm repository

Currently, we recommend installing Kubernetes monitoring using the tool `helm`,
as it is also suitable for less experienced users and
standardizes the management of configurations.
Helm is a kind of a package manager for Kubernetes.
You can use it to include _repositories_ as sources and
easily add the Helm charts they contain like packages to your cluster.
To do this, first of all make the repository known.
In the following example,
we use the name `tribe29` to make it easier to access the repository later.
However, you can of course use any other name:

=== Adjustments to the configuration

With Helm, you create the necessary configuration files completely on your own.
In order to be able to determine certain parameters over all configurations,
you give thereby a control file along, the so-called `values.yaml`.
As a starting point we recommend
the link:https://raw.githubusercontent.com/tribe29/checkmk_kube_agent/main/deploy/charts/checkmk/values.yaml[template^] provided by us.
Copy it and adapt it to your own environment.

Since we cannot know in advance how your Kubernetes cluster is set up,
we have chosen the safest option for how the {CMK} collectors are started:
By default, they do not expose any ports to be reached from the outside.
To allow you to access the collectors later, adjust these settings accordingly.

For simplicity, let's take our template as a starting point.
We support two communication paths by default: the query via _Ingress_ and the query via _NodePort_.
Depending on which variant you support in your cluster, the configuration will vary.

==== Provide communication via Ingress

If you use link:https://kubernetes.io/docs/concepts/services-networking/ingress/[Ingress^] to control access to your services, adjust the already prepared parts in `values.yaml` accordingly.
For a better overview only the relevant part is shown in the following example.
Set the value `enabled` to `true`. You adjust the remaining values according to your environment:

[{yaml}]
----
  ingress:
    enabled: true
    className: ""
    annotations:
      nginx.ingress.kubernetes.io/rewrite-target: /
    hosts:
      - host: checkmk-cluster-collector.local
        paths:
          - path: /
            pathType: Prefix
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
----

==== Provide communication via NodePort

You can also provide access to the services directly through a port.
This is necessary if you do not use Ingress.
Also in the following example only the relevant section is shown.
You set the value `type` to `NodePort` and remove the comment for the value `nodePort`:

[{yaml}]
----
  service:
    # if required specify "NodePort" here to expose the cluster-collector via the "nodePort" specified below
    type: NodePort
    port: 8080
    nodePort: 30035
----

=== Let create the configuration files

After customizing `values.yaml` or creating your own,
use the following command to create all the necessary configuration files
to set up your Kubernetes cluster for monitoring in {CMK}:

[{shell}]
----
{c-user} helm upgrade --install --create-namespace -n cmk-monitoring checkmk tribe29/checkmk -f values.yaml
----

Since the command is not self-explanatory, we provide an explanation of each option below:

[cols="30,~",options="header"]
|===
|command part |meaning
|`helm upgrade --install` |This part is the basic command to send the configuration to the Kubernetes cluster.
|`--create-namespace` |In Kubernetes you always specify to which _namespace_ the configuration should be added. You need this option if the namespace does not exist yet. Helm will create it in this case.
|`-n cmk-monitoring` |This option specifies the namespace to which the configuration should be added. cmk-monitoring` is just an example of what it could be called.
|`checkmk` |`checkmk` is an example for the name of the helmet chart. Ideally, you should leave this name as it is, because only then you will automatically benefit from the fact that Kubernetes objects get short names.
|`tribe29/checkmk` |The first part of this option describes the repository you created with the command before. The second part -- after the slash -- is the _package_ where the necessary information is located to be able to create the configuration of your Kubernetes monitoring.
|`-f values.yaml` |Finally, specify the configuration file that you created or customized earlier. It contains all the customizations to be included in the configuration files created with `helm`.
|===

After you run the command, your Kubernetes cluster is prepared to be monitored with {CMK}.
The cluster will now take care of itself to ensure that the necessary pods and the containers within them are running and accessible.

=== Alternative: Set up via manifest

Normally, it does not make sense for you to customize the manifests (configuration files) yourself.
On the one hand,
because you need detailed knowledge of the architecture of the _{CMK} Kubernetes Collectors_ for this and,
on the other hand,
because manual customization is much more error-prone.
For example, in `helm` you set up communication over TLS once,
instead of adding it to all relevant places in the manifests themselves.

However, if you don`t use `helm`, or want to have control over all the setup details,
you can still go this route.

To do so, first download the manifests we have pre-built from our corresponding link:https://github.com/tribe29/checkmk_kube_agent/tree/main/deploy/kubernetes[repository at GitHub^].
We have split the whole configuration into several files to facilitate their maintenance or to provide more concise files for clearly defined purposes.

You need at least the following five files:

[cols="30,~",options="header"]
|===
|filename |purpose
|`00_namespace.yaml` |Create namespace named checkmk-monitoring
|`checkmk-serviceaccount.yaml` |Create service account named checkmk and cluster role named checkmk-metrics-reader in namespace checkmk-monitoring
|`cluster-collector.yaml` |Here we create the cluster collector we have named. Among other things, a service account named cluster-collector is created in the namespace checkmk-monitoring and the service accounts are assigned roles within the cluster. In addition, the deployment named cluster-collector is defined.
|`node-collector.yaml` |Analogous to `cluster-collector.yaml` for the nodes
|`service.yaml` | Create service named cluster-collector in namespace checkmk-monitoring. Create a service named cluster-collector-nodeport in the namespace checkmk-monitoring. The port for the NodePort is also specified here.
|===

If you don't want to clone the whole repo right away - which you are free to do, of course - you can use the following command to download specifically the five files you need:

[{shell}]
----
{c-user} URL='\https://raw.githubusercontent.com/tribe29/checkmk_kube_agent/main/deploy/kubernetes/'; for i in 00_namespace checkmk-serviceaccount cluster-collector node-collector service; do wget "${URL}${i}.yaml"; done
----

If you also want to set up a network policy and a pod security policy, you also need the following two files:

 network-policy.yaml
 pod-security-policy.yaml

[{shell}]
----
{c-user} URL='\https://raw.githubusercontent.com/tribe29/checkmk_kube_agent/main/deploy/kubernetes/'; for i in network-policy pod-security-policy; do wget "${URL}${i}.yaml"; done
----

In the files `cluster-collector.yaml` and `node-collector.yaml` you have to fill four placeholders with concrete content.
In both files you will find places where `main_<YYY.MM.DD>` is written.
Replace these placeholders with tags from our link:https://hub.docker.com/r/checkmk/kubernetes-collector/tags[Kubernetes collector on Docker Hub^].
For example, you could use the following command to replace all occurrences of `main_<YYY.MM.DD>` with the March 1, 2022 build tag of our container.

[{shell}]
----
{c-user} sed -i 's/main_<YYYY.MM.DD>/main_2022.03.01/g' node-collector.yaml cluster-collector.yaml
----

For the communication to the outside a service of the type NodePort is needed.
This allows communication from the outside and is also permanently set to TCP port 30035 in the `service.yaml` file.
If this port is already assigned in your cluster, please change the port accordingly.

Once you have made these settings, you can apply these manifest files collectively to your cluster.
To do this, run the following command from the manifest location:

[{shell}]
----
{c-user} kubectl apply -f .
namespace/checkmk-monitoring created
serviceaccount/checkmk created
clusterrole.rbac.authorization.k8s.io/checkmk-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/checkmk-metrics-reader-binding created
serviceaccount/cluster-collector created
clusterrolebinding.rbac.authorization.k8s.io/checkmk-cluster-collector created
clusterrolebinding.rbac.authorization.k8s.io/checkmk-token-review created
deployment.apps/cluster-collector created
serviceaccount/node-collector-machine-sections created
serviceaccount/node-collector-container-metrics created
clusterrole.rbac.authorization.k8s.io/node-collector-container-metrics-clusterrole created
podsecuritypolicy.policy/node-collector-container-metrics-podsecuritypolicy created
clusterrolebinding.rbac.authorization.k8s.io/node-collector-container-metrics-cluterrolebinding created
daemonset.apps/node-collector-container-metrics created
daemonset.apps/node-collector-machine-sections created
service/cluster-collector created
service/cluster-collector-nodeport created
----

You can also use `kubectl` to check whether the manifests have been applied correctly.
To do this, use the following command to display all the pods in the `checkmk-monitoring` namespace:

[{shell}]
----
{c-user} kubectl get pods -n checkmk-monitoring
----

Furthermore, you can also check all services within the namespace as follows:

[{shell}]
----
{c-user} kubectl get svc -n checkmk-monitoring
----


== Set up the monitoring in {CMK}

Next, in the GUI of {CMK}, we move on to setting up the xref:glossar#special_agent[specialagent] and a rule to automatically create hosts for your Kubernetes objects.
However, to set up the special agent, there are a few prerequisites that need to be met first:


[#token]
=== Store password (token) in {CMK}

The best way to store the password (token) of the service account is to store it in the password store of {CMK}.
This is the most secure variant, because you can separate the storage and use of the password organizationally.
Alternatively, enter it directly in plain text when creating the rule (see below).

If you have kept the default `checkmk-monitoring` as the namespace for monitoring your Kuberentes cluster, the following command line will cut the password directly from the output of `kubectl get secrets`:

[{shell}]
----
{c-user} kubectl get secret $(kubectl get serviceaccount checkmk -o=jsonpath='{.secrets[*].name}' -n checkmk-monitoring) -n checkmk-monitoring -o=jsonpath='{.data.token}' | base64 --decode
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJjaGVjay1tayIsI
mt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjaGVjay1tay10b2tlbi16OWhicCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5
hbWUiOiJjaGVjay1tayIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjIxODE3OWEzLTFlZTctMTFlOS1iZjQzLTA4MDAyN2E1ZjE0MSIsInN1YiI6I
nN5c3RlbTpzZXJ2aWNlYWNjb3VudDpjaGVjay1tazpjaGVjay1tayJ9.gcLEH8jjUloTeaAj-U_kRAmRVIiETTk89ujViriGtllnv2iKF12p0L9ybT1fO-1Vx7XyU8jneQRO9lZw8JbhVmaPjrkEc8
kAcUdpGERUHmVFG-yj3KhOwMMUSyfg6wAeBLvj-y1-_pMJEVkVbylYCP6xoLh_rpf75JkAicZTDmhkBNOtSf9ZMjxEmL6kzNYvPwz76szLJUg_ZC636OA2Z47qREUtdNVLyutls7ZVLzuluS2rnfoP
JEVp_hN3PXTRei0F5rNeA01wmgWtDfo0xALZ-GfvEQ-O6GjNwHDlsqYmgtz5rC23cWLAf6MtETfyeEJjRqwituhqUJ9Jp7ZHgQ%
----

The password is really that long.
If you work directly under Linux, you can add a `| xsel --clipboard` at the end.
Then the password is not printed at all, but copied directly to the clipboard (as if you had copied it with the mouse):

[{shell}]
----
{c-user} kubectl get secret $(kubectl get serviceaccount checkmk -o=jsonpath='{.secrets[*].name}' -n checkmk-monitoring) -n checkmk-monitoring -o=jsonpath='{.data.token}' | base64 --decode | xsel --clipboard
----

Add the password to the {CMK} password store with [.guihint]#Setup > General > Passwords > Add password# e.g. under the ID and the title `kubernetes`:

image::kubernetes_password.png[]


[#certimport]
=== Import CA of the service account into {CMK}

In order for {CMK} to trust the Certificate Authority (CA) of the service account, you must store the CA certificate in {CMK}. You can read out the certificate - provided you have kept `checkmk-monitoring` as the namespace - with the following command:

[{shell}]
----
{c-user} kubectl get secret $(kubectl get serviceaccount checkmk -o=jsonpath='{.secrets[*].name}' -n checkmk-monitoring) -n checkmk-monitoring -o=jsonpath='{.data.ca\.crt}' | base64 --decode
----

Copy everything here including the lines `BEGIN CERTIFICATE` and `END CERTIFIACTE` and add the certificate in the Setup menu under [.guihint]#Setup > General > Global settings > Site management > Trusted certificate authorities for SSL#.

image::kubernetes_ca.png[]

[#source-host]
=== Create Piggyback source host

Create a new host in Checkmk in the usual way and name it `mykubernetesclusterhost`, for example.
As the title and host name suggest, this host is used to collect the Piggyback data and also to map all services and metrics at the cluster level.
Since this host only receives data via the special agent, set the [.guihint]#IP address family# option to [.guihint]#No IP#.

=== Set up dynamic host configuration

To ensure separation between the numerous Kubernetes objects and the rest of your monitoring environment, it is a good idea to first create a folder via [.guihint]#Setup > Hosts > Add folder# in which the xref:dcd#[dynamic host configuration] can automatically create all necessary hosts.
Creating or using such a folder is certainly to be considered optional.

However, it is absolutely necessary to set up a connector for the piggyback data. Via [.guihint]#Setup > Hosts > Dynamic host management > Add connection# you get to the page for the corresponding setup.
First enter a title and then click [.guihint]#show more# under [.guihint]#Connection Properties#.

Next, click [.guihint]#Add new element# and under [.guihint]#Create hosts in# select the folder you created earlier.

In a Kubernetes environment, where monitorable and monitored objects naturally come and go, it is also recommended to enable the [.guihint]#Automatically delete hosts without piggyback data# option.
What exactly this option does and under what circumstances hosts are then actually deleted is explained in the section xref:dcd#_automatically_delete_hosts[Automatically deleting hosts] in the article on dynamic host configuration.

Now enter the previously created xref:source-host[Piggyback source host] under [.guihint]#Restrict source hosts# and enable the [.guihint]#Discover services during creation# option.

The [.guihint]#Connection Properties# section of this new connector might look like the following afterwards:

image::monitoring_kubernetes_connection_properties.png[alt="Sample dynamic host configuration settings."]

[#rule]
=== Setting up the special agent

Now that all the prerequisites are in place in the cluster and in {CMK}, you can turn your attention to configuring the special agent.
This can be found via [.guihint]#Setup > Agents > VM, Cloud, Container > Kubernetes#.

First of all, you need to assign a name for the cluster you want to monitor.
You can choose this name freely.
It is used to give a unique name to all objects that originate from exactly this cluster.
For example, if you enter `mycluster` here, the names of the hosts of all pods from this cluster will later start with `pod_mycluster`.
The next part of the host name will then always be the namespace in which this Kubernetes object exists.

Under [.guihint]#Token#, now select the xref:token[previously created entry] from the password store of {CMK}.

Under [.guihint]#API server connection > Endpoint# {CMK} now requires the entry of the URL (or IP address) via which your Kubernetes API server can be reached.
The port must also be specified here if the service was not provided via a virtual host.
The easiest way to find out this IP address -- if you don't already have it handy -- depends on your Kubernetes environment.
The following command will give you the endpoint of the API server as IP address and port, which you will find as the last entry under `server` in the shortened output:

[{shell}]
----
{c-user} kubectl config view
apiVersion: v1
clusters:
  - cluster:
    certificate-authority-data: DATA+OMITTED
    server: \https://10.73.42.21:6443
    name: my-kubernetes
----

If the server is provided via a DNS record, the output will look more like this instead:

[{shell}]
----
{c-user} kubectl config view
apiVersion: v1
clusters:
  - cluster:
    certificate-authority-data: DATA+OMITTED
    server: \https://DFE7A4191DCEC150F63F9DE2ECA1B407.mi6.eu-central-1.eks.amazonaws.com
    name: xyz:aws:eks:eu-central-1:150143619628:cluster/my-kubernetes
----

If you have stored the CA of your cluster - xref:certimport[as described above] - in {CMK}, you can select [.guihint]#Verify the certificate# under [.guihint]#SSL certificate verification#.

If your Kubernetes API server is only accessible via a proxy or special timeouts are required for the connection, you can enter them under [.guihint]#HTTP proxy# and [.guihint]#TCP timeouts#.

Next, you have the choice to enrich the monitoring of your Kubernetes cluster with usage data collected by the {CMK} cluster collector.
To do this, you need to specify the protocol, URL, and port of the cluster collector under [.guihint]#Collector NodePort/Ingress endpoint#.
If you set it up using our manifests, the port here is `30035` by default.
If you have customized the port in the `service.yaml` file, change the port here accordingly.
You should be able to find out the URL or IP address of the nodeport from the description of the `cluster-collector` pod.
Just run the following command and look in the output in the line starting with `Node:`:

[{shell}]
----
{c-user} kubectl describe pod $(kubectl get pods --no-headers -o custom-columns=":metadata.name") | grep -A5 Name:.*cluster-collector
Name:         cluster-collector-5b7c8468cf-5t5hj
Namespace:    checkmk-monitoring
Priority:     0
Node:         minikube/[green]#172.16.23.2#
Start Time:   Wed, 03 Mar 2022 20:54:45 +0100
Labels:       app=cluster-collector
----

With the options [.guihint]#Collect information about...# you can now finally select which objects within your cluster should be monitored.
Our preselection covers the most relevant objects.
If you decide to monitor the [.guihint]#Pods of CronJobs# as well, please refer to the xref:user_interface#inline_help[inline help] for this point.

Last but not least, you can choose whether you want to monitor only certain namespaces within your clusters or whether explicit namespaces should be excluded from monitoring.
You specify this using the [.guihint]#Monitor namespaces# option.

Your rule might now look like the following:

image::monitoring_kubernetes_rule.png[alt="Exemplary completed rule for Kubernetes special agent."]

*Important:* Under [.guithint]#Conditions > Explicit hosts# you must now re-enter the xref:source-host[previously created host]:

image::monitoring_kubernetes_explicit_hosts.png[alt="Rules for special agents must always be set to explicit hosts, as seen here."]

Next, store the rule and perform a service discovery on this host.
You will see the first cluster-level services right here:

image::monitoring_kubernetes_service_discovery.png[alt="Exemplary view of the first service discovery after the configuration is complete."]

Afterwards, activate all the changes you made and let the dynamic host configuration do the work from now on.
It will generate all hosts for your Kubernetes objects after a short time.

== Labels for Kubernetes objects

{CMK} automatically generates labels for Kubernetes objects such as clusters, deployments, or namespaces during service discovery.
All labels to Kubernetes objects that {CMK} automatically generates start with `cmk/kubernetes/`.
For example, a pod always gets a label of the node (`cmk/kubernetes/node:mynode`), a label that just shows that this object is a pod (`cmk/kubernetes/object:pod`) and a label for the namespace (`cmk/kubernetes/namespace:mynamespace`).
This makes it very easy to create filters and rules for all objects of the same type or in the same namespace.

== Hardware/Software Inventory

Kubernetes monitoring of {CMK} also supports xref:inventory#[HW/SW inventory].

image::kubernetes_monitoring_hw_sw_inventory.png[alt="Exemplary view of hardware and software inventory of a pod."]

== Removing {CMK}

If you have deployed {CMK} to your cluster via our manifests, you can remove created accounts, service and so on just as easily as they were set up.
To do this, go back to the directory where the YAML files are located and run the following command:

[{shell}]
----
{c-user} kubectl delete -f .
namespace "checkmk-monitoring" deleted
serviceaccount "checkmk" deleted
clusterrole.rbac.authorization.k8s.io "checkmk-metrics-reader" deleted
clusterrolebinding.rbac.authorization.k8s.io "checkmk-metrics-reader-binding" deleted
serviceaccount "cluster-collector" deleted
clusterrolebinding.rbac.authorization.k8s.io "checkmk-cluster-collector" deleted
clusterrolebinding.rbac.authorization.k8s.io "checkmk-token-review" deleted
deployment.apps "cluster-collector" deleted
serviceaccount "node-collector-machine-sections" deleted
serviceaccount "node-collector-container-metrics" deleted
clusterrole.rbac.authorization.k8s.io "node-collector-container-metrics-clusterrole" deleted
podsecuritypolicy.policy "node-collector-container-metrics-podsecuritypolicy" deleted
clusterrolebinding.rbac.authorization.k8s.io "node-collector-container-metrics-cluterrolebinding" deleted
daemonset.apps "node-collector-container-metrics" deleted
daemonset.apps "node-collector-machine-sections" deleted
service "cluster-collector" deleted
service "cluster-collector-nodeport" deleted
----
