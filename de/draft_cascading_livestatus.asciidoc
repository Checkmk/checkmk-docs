// -*- coding: utf-8 -*-
include::global_attr.adoc[]
= Cascading Livestatus
:revdate: draft
:title: Cascading Livestatus
:description: Hier erfahren Sie, wie Sie mit {CMK} ein verteiltes Monitoring mit einer Zentralinstanz und mehreren Remote-Instanzen einrichten können.

{related-start}
xref:livestatus#[Statusdaten abrufen via Livestatus]
xref:cmc#[Der {CMK} Micro Core]
xref:omd_basics#[Instanzen (Sites) mit omd verwalten]
{related-end}


////
// ML: Schön wäre hier eine Erweiterung des obigen Schaubilds um einige Viewer-Instanzen. Das Hin und Her mit drei Instanzarten ist sonst vielleicht verwirrend.
// ML: Beim Review bitte mal gucken, ob das Beispiel mit den Kundennetzen und dem Management nicht allzu abwegig ist.
// ML: Nur für das Review ein eigener Artikel, danach Integration in distributed_monitoring.

// ML: Einleitung:

Ende von 2.1 erweitern:

Ende:
So ein System hat nicht nur eine gemeinsame Statusoberfläche, sondern auch eine gemeinsame Konfiguration und fühlt sich dann endgültig wie „ein großes System“ an.

Neu:
Sie können solch ein System sogar noch um reine Viewer-Instanzen erweitern, die nur als Statusoberflächen dienen, etwa für Teilbereiche oder bestimmte Nutzergruppen.


// ML: Neues Kapitel 4 zwischen "Instabile ..." und "Livedump ..."

=== Cascading Livestatus
Wie in der Einleitung bereits erwähnt, ist es möglich, das verteilte Monitoring um reine {CMK}-Viewer-Instanzen zu erweitern, die die Monitoring-Daten von Remote-Instanzen anzeigen, die selbst nicht direkt erreichbar sind.
Einzige Voraussetzung dafür: Die Zentralinstanz muss natürlich erreichbar sein.
Technisch wird dies über den Livestatus-Proxy umgesetzt: Die Zentralinstanz bekommt via Livestatus Daten von der Remote-Instanz und fungiert selbst als Proxy -- kann die Daten also an dritte Instanzen durchreichen.
Diese Kette können Sie beliebig erweitern, also zum Beispiel eine zweite Viewer-Instanz an die erste anbinden.

Praktisch ist dies beispielsweise für ein Szenario wie folgendes: Die Zentralinstanz kümmert sich um drei unabhängige Netze _kunde1, kunde2 und kunde3_ und wird selbst im Netz _betreiber1_ betrieben.
Möchte nun etwa das Management des Betreibers aus dem Netz _betreiber1_ den Monitoring-Status der Kundeninstanzen sehen, könnte dies natürlich über einen Zugang auf der Zentralinstanz selbst geregelt werden.
Aus technischen wie rechtlichen Gründen könnte die Zentralinstanz jedoch ausschließlich den zuständigen Mitarbeitern vorbehalten sein.
Über eine rein zur Ansicht von entfernten Instanzen aufgesetzte Viewer-Instanz lässt sich der direkte Zugriff umgehen.
Die Viewer-Instanz zeigt dann zwar Hosts und Services verbundener Instanzen, die eigene Konfiguration bleibt jedoch komplett leer.

Das Aufsetzen erfolgt in den Verbindungseinstellungen des verteilten Monitorings, zunächst auf der Zentralinstanz, dann auf der Viewer-Instanz (welche noch nicht existieren muss).

Öffnen Sie auf der Zentralinstanz die Verbindungseinstellungen der gewünschten Remote-Instanz über [.guihint]#Setup > General > Distributed monitoring.#
Unter [.guihint]#Use Livestatus Proxy Daemon# aktivieren Sie die Option [.guihint]#Allow access via TCP# und geben einen freien Port an (hier `6560`).
So verbindet sich der Livestatus-Proxy der Zentralinstanz mit der Remote-Instanz und öffnet einen Port für Anfragen der Viewer-Instanz -- welche dann an die Remote-Instanz weitergeleitet werden.

image::distributed_livestatus_cascading_master.png[alt="Verbindungseinstellungen im verteilten Monitoring mit aktiviertem TCP-Zugriff für Viewer-Instanzen."]

Erstellen Sie nun eine Viewer-Instanz und öffnen Sie auch dort wieder die Verbindungseinstellungen über [.guihint]#Setup > General > Distributed monitoring.#
In den [.guihint]#Basic settings# geben Sie -- wie immer bei Verbindungen im verteilten Monitoring -- als [.guihint]#Site ID# den exakten Namen der Zentralinstanz an.

image::distributed_livestatus_cascading_viewer_01.png[alt="Instanz-ID in den Verbindungseinstellungen."]

Im Bereich [.guihint]#Status connection# geben Sie als Host die Zentralinstanz an -- sowie den manuell vergebenen freien Port (hier `6550`).

image::distributed_livestatus_cascading_viewer_02.png[alt="Verbindungseinstellungen mit Zentralinstanz und freiem Port für Livestatus."]

Sobald die Verbindung steht, sehen Sie die gewünschten Hosts und Services der Remote-Instanz in den Monitoring-Ansichten der Viewer-Instanz.

Möchten Sie weiter kaskadieren, müssen Sie auf der Viewer-Instanz, wie zuvor auf der Zentralinstanz, ebenfalls den TCP-Zugriff auf den Livestatus-Proxy erlauben, erneut mit einem anderen, freien Port.
////











