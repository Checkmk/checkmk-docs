include::global_attr.adoc[]
= Kubernetes überwachen
:revdate: draft
:title: Kubernetes überwachen
:description: Mit {CMK} können Sie auch die Container-Orchestrierungssoftware Kubernetes überwachen. Lesen Sie hier die Details zu der agentenlosen Einrichtung.

{related-start}
xref:wato_monitoringagents#[Monitoring-Agenten]
xref:monitoring_docker#[Docker überwachen]
link:https://checkmk.com/de/integrations[Katalog der Check-Plugins^]
{related-end}



== Einleitung

[{image-left}]
image::kubernetes_logo.jpg[width=140]

Der große Erfolg von xref:monitoring_docker#[Docker] hat dazu geführt, dass Leute Docker in immer größerem Maßstab einsetzen.
Der im Gegensatz zu virtuellen Maschinen à la xref:monitoring_vmware#[VMWare] sehr geringe Overhead macht den Container „billig“ und damit quasi zur Massenware.
Dass dabei ein gutes Werkzeug für die Orchestrierung der Container essentiell ist, versteht sich von selbst.
Für die Mehrheit ist das Open Source Tool link:https://de.wikipedia.org/wiki/Kubernetes[Kubernetes^] das Mittel der Wahl.

*Wichtig*: Die Kubernetes Versionen 1.18 und neuer werden aktuell nur eingeschränkt unterstützt, da der offizielle Kubernetes Python Client, welcher mit den neueren Versionen der API kompatibel ist, schlicht noch nicht vorliegt.

Quelle: link:https://github.com/kubernetes-client/python[Kubernetes Python Client @ github^]

RFC: link:https://github.com/kubernetes-client/python/issues/1242[Release Kubernetes Python Clients in parallel for released Kubernetes versions^]

{CMK} unterstützt die Überwachung von Kubernetes.
Der Schwerpunkt liegt aktuell dabei auf Zuständen und Metriken, die vor allem für den Administrator interessant sind.
Folgende Check-Plugins sind verfügbar:

* link:https://checkmk.com/de/integrations/k8s_component_statuses[Kubernetes: Component Status^]
* link:https://checkmk.com/de/integrations/k8s_conditions[Kubernetes: Conditions^]
* link:https://checkmk.com/de/integrations/k8s_namespaces[Kubernetes: Namespaces^]
* link:https://checkmk.com/de/integrations/k8s_nodes[Kubernetes: Nodes^]
* link:https://checkmk.com/de/integrations/k8s_persistent_volume_claims[Kubernetes: Persistent Volume Claims^]
* link:https://checkmk.com/de/integrations/k8s_persistent_volumes[Kubernetes: Persistent Volumes^]
* link:https://checkmk.com/de/integrations/k8s_pods_cpu[Kubernetes: Pods CPU Resources^]
* link:https://checkmk.com/de/integrations/k8s_pods_fs[Kubernetes: Pods Filesystem Resources^]
* link:https://checkmk.com/de/integrations/k8s_pods_memory[Kubernetes: Pods Memory Resources^]
* link:https://checkmk.com/de/integrations/k8s_resources_cpu[Kubernetes: CPU Resources^]
* link:https://checkmk.com/de/integrations/k8s_resources_memory[Kubernetes: Memory Resources^]
* link:https://checkmk.com/de/integrations/k8s_resources_pods[Kubernetes: Pod Resources^]
* link:https://checkmk.com/de/integrations/k8s_roles[Kubernetes: Roles^]
* link:https://checkmk.com/de/integrations/k8s_storage_classes[Kubernetes: Storage Classes^]
* link:https://checkmk.com/de/integrations/k8s_daemon_pods[Kubernetes: Daemon Pods^]
* link:https://checkmk.com/de/integrations/k8s_pod_container[Kubernetes: Pod Container Statistics^]
* link:https://checkmk.com/de/integrations/k8s_replicas[Kubernetes: Replicas^]
* link:https://checkmk.com/de/integrations/k8s_service_port[Kubernetes: Service Ports^]
* link:https://checkmk.com/de/integrations/k8s_stateful_set_replicas[Kubernetes: Replicas (set)^]
* link:https://checkmk.com/de/integrations/k8s_stats_fs[Kubernetes: Node and Cluster Level Filesystem Usage^]
* link:https://checkmk.com/de/integrations/k8s_stats_network[Kubernetes: Node and Cluster Level Network Usage^]


[#setup]
== Einrichten der Überwachung

=== Service-Account

Um einen Kubernetes-Cluster in {CMK} einzurichten, benötigen Sie zunächst einen Service-Account und eine damit verbundene Cluster-Rolle in Kubernetes, damit {CMK} auf die API zugreifen kann.
Wir stellen für Sie als Schablone die Datei `check_mk_rbac.yaml` bereit.
Diese finden Sie den „Treasures“ im Verzeichnis `share/doc/check_mk/treasures/kubernetes` oder online
link:https://github.com/tribe29/checkmk/blob/master/doc/treasures/kubernetes/check_mk_rbac.yaml[hier^].
Deren *Anfang* sieht etwa so aus:

.share/doc/check_mk/treasures/kubernetes/check_mk_rbac.yaml
[{file}]
----
---
apiVersion: v1
kind: Namespace
metadata:
  name: check-mk
---
kind: ServiceAccount
----

Wir verwenden hier als Name und Namespace jeweils `check-mk`.

Spielen Sie diese Datei auf Ihrem Kubernetes-Cluster mit dem Befehl `kubectl` ein:

[{shell}]
----
{c-user} kubectl apply -f check_mk_rbac.yaml
namespace/check-mk created
serviceaccount/check-mk created
clusterrole.rbac.authorization.k8s.io/check-mk created
clusterrolebinding.rbac.authorization.k8s.io/check-mk created
----

Falls Sie die Google Kubernetes Engine verwenden, kann es sein, dass Sie den Fehler `"Error from server (Forbidden): error when creating
"check_mk_rbac.yaml":` bekommen.
In diesem Fall müssen Sie zuvor die Berechtigungen Ihres Benutzers erweitern.
Das geht mit folgendem Befehl (wobei Sie `MYNAME` durch Ihren Benutzernamen bei Google ersetzen):

[{shell}]
----
{c-user} kubectl create clusterrolebinding MYNAME-cluster-admin-binding --clusterrole=cluster-admin --user=MYNAME@example.org
----

// Hier fehlt:
// - Der korrekte Prompt. Ist das root? oder wie heißt der User normalerweise?
// - Die Ausgabe des Befehls

Wenn alles gut gegangen ist, können Sie den neuen Service-Account mit `kubectl get serviceaccounts` abfragen:

[{shell}]
----
{c-user} kubectl get serviceaccounts check-mk -n check-mk -o yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"name":"check-mk","namespace":"check-mk"}}
  creationTimestamp: "2019-01-23T08:16:05Z"
  name: check-mk
  namespace: check-mk
  resourceVersion: "4004661"
  selfLink: /api/v1/namespaces/check-mk/serviceaccounts/check-mk
  uid: 218179a3-1ee7-11e9-bf43-080027a5f141
secrets:
- name: #check-mk-token-z9hbp#
----

Dort finden Sie dann auch den Namen des zugehörigen Secrets.
Dies hat die Form `check-mk-token-<ID>` (hier im Beispiel `check-mk-token-z9hbp`).
Die ID für das Secret wird von Kubernetes automatisch generiert.
Den Inhalt des Secrets können Sie anschließend mit `get secrets` abfragen:

[{shell}]
----
{c-user} kubectl get secrets check-mk-token-z9hbp -n check-mk -o yaml
apiVersion: v1
data:
*  ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQVRBTkJna3Foa2lHO...*
  namespace: Y2hlY2stbWs=
*  token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNklpSjkuZXlKcGMzTWlPaUpyZFdKbGNtNWxkR1Z6TDNObG...*
kind: Secret
metadata:
  annotations:
    kubernetes.io/service-account.name: check-mk
    kubernetes.io/service-account.uid: 218179a3-1ee7-11e9-bf43-080027a5f141
  creationTimestamp: "2019-01-23T08:16:06Z"
  name: check-mk-token-z9hbp
  namespace: check-mk
  resourceVersion: "4004660"
  selfLink: /api/v1/namespaces/check-mk/secrets/check-mk-token-z9hbp
  uid: 2183cee6-1ee7-11e9-bf43-080027a5f141
type: kubernetes.io/service-account-token
----

In der Ausgabe ist unter anderem das base64-kodierte CA-Zertifikat (`ca.crt`) und das base64-kodierte Token (`token`) für den Account enthalten.
Sie können das Zertikat aus der Ausgabe von `get secrets` z.B. mit folgendem Befehl ausschneiden und gleich in die Form umwandeln, die Sie für den Import in {CMK} benötigen:

[{shell}]
----
{c-user} kubectl get secrets check-mk-token-z9hbp -n check-mk -o jsonpath='{.data.ca\.crt}' | base64 --decode
-----BEGIN CERTIFICATE-----
MIIC5zCCAc+gAwIBAgIBATANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwptaW5p
a3ViZUNBMB4XDTE4MDkxMDE2MDAwMVoXDTI4MDkwODE2MDAwMVowFTETMBEGA1UE
AxMKbWluaWt1YmVDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAK9Z
iG0gNZK5VU94a0E6OrUqxOQRdkv6S6vG3LnuozdgNfxsEetR9bMGu15DWaSa40JX
FbC5RxzNq/W9B2pPmkAlAguqHvayn7lNWjoF5P+31tucIxs3AOfBsLetyCJQduYD
jbe1v1/KCn/4YUzk99cW0ivPqnwVHBoMPUfVof8yA00RJugH6lMZL3kmOkD5AtRH
FTThW9riAlJATBofLfkgRnUEpfb3u1xF9vYEDwKkcV91ealZowJ/BciuxM2F8RIg
LdwF/vOh6a+4Cu8adTyQ8mAryfVPDhFBhbsg+BXRykhNzNDPruC+9wAG/50vg4kV
4wFpkPOkOCvB8ROYelkCAwEAAaNCMEAwDgYDVR0PAQH/BAQDAgKkMB0GA1UdJQQW
MBQGCCsGAQUFBwMCBggrBgEFBQcDATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3
DQEBCwUAA4IBAQAeNwON8SACLl2SB8t8P4/heKdR3Hyg3hlAOSGjsyo396goAPS1
t6IeCzWZ5Z/LsF7o8y9g8A7blUvARLysmmWOre3X4wDuPvH7jrYt+PUjq+RNeeUX
5R1XAyFfuVcWstT5HpKXdh6U6HfzGpKS1JoFkySrYARhJ+MipJUKNrQLESNqdxBK
4gLCdFxutTTFYkKf6crfIkHoDfXfurMo+wyEYE4Yeh8KRSQWvaKTdab4UvMwlUbO
+8wFZRe08faBqyvavH31KfmkBLZbMMM5r4Jj0Z6a56qZDuiMzlkCl6rmKynQeFzD
KKvQHZazKf1NdcCqKOoU+eh6q6dI9uVFZybG
-----END CERTIFICATE-----
----


[#certimport]
=== Zertifikat in {CMK} importieren

Damit {CMK} dem CA-Zertifikat von Kubernetes vertraut, müssen Sie dieses im Setup-Menü unter [.guihint]#Setup > General > Global settings > Site Management > Trusted certificate authorities for SSL# hinzufügen:

image::kubernetes_ca.png[]

Ohne den korrekten Import der CA wird später der {CMK}-Service des Kubernetes-Clusters mit `bad handshake` und `certificate verify failed` fehlschlagen:

image::kubernetes_ssl_error.png[]


[#token]
=== Passwort (Token) in {CMK} hinterlegen

Das Passwort (Token) des Service-Accounts können Sie nun am besten im Passwortspeicher von {CMK} hinterlegen.
Das ist die sicherste Variante, da Sie Hinterlegung und Benutzung des Passworts organisatorisch trennen können.
Alternativ geben Sie es beim Anlegen der Regel (siehe weiter unten) direkt im Klartext ein.

Folgende Befehlszeile schneidet das Passwort direkt aus der Ausgabe von `get secrets` aus:

[{shell}]
----
{c-user} kubectl get secrets check-mk-token-z9hbp -n check-mk -o jsonpath='{.data.token}' | base64 --decode
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJjaGVjay1tayIsI
mt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjaGVjay1tay10b2tlbi16OWhicCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5
hbWUiOiJjaGVjay1tayIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjIxODE3OWEzLTFlZTctMTFlOS1iZjQzLTA4MDAyN2E1ZjE0MSIsInN1YiI6I
nN5c3RlbTpzZXJ2aWNlYWNjb3VudDpjaGVjay1tazpjaGVjay1tayJ9.gcLEH8jjUloTeaAj-U_kRAmRVIiETTk89ujViriGtllnv2iKF12p0L9ybT1fO-1Vx7XyU8jneQRO9lZw8JbhVmaPjrkEc8
kAcUdpGERUHmVFG-yj3KhOwMMUSyfg6wAeBLvj-y1-_pMJEVkVbylYCP6xoLh_rpf75JkAicZTDmhkBNOtSf9ZMjxEmL6kzNYvPwz76szLJUg_ZC636OA2Z47qREUtdNVLyutls7ZVLzuluS2rnfoP
JEVp_hN3PXTRei0F5rNeA01wmgWtDfo0xALZ-GfvEQ-O6GjNwHDlsqYmgtz5rC23cWLAf6MtETfyeEJjRqwituhqUJ9Jp7ZHgQ%
----

Das Passwort ist wirklich so lang.
Wenn Sie direkt unter Linux arbeiten, können Sie hinten noch ein `| xsel --clipboard` hinzufügen.
Dann wird das Passwort gar nicht ausgegeben, sondern direkt in die Zwischenablage kopiert (also als ob Sie das mit der Maus kopiert hätten):

[{shell}]
----
{c-user} kubectl get secrets check-mk-token-z9hbp -n check-mk -o jsonpath='{.data.token}' | base64 --decode | xsel --clipboard
----

Fügen Sie das Passwort in den {CMK}-Passwortspeicher ein mit [.guihint]#Setup > General > Passwords > Add password# z.B. unter der ID `kubernetes`:

image::kubernetes_password.png[]


[#rule]
=== Kubernetes-Cluster ins Monitoring aufnehmen

Die Überwachung von {CMK} geschieht auf zwei Ebenen.
Der Kubernetes-Cluster selbst wird als ein Host überwacht.
Für die einzelnen Kubernetes-Knoten (_nodes_) verwenden wir das xref:piggyback#[Piggyback-Prinzip].
Das bedeutet, dass jeder Knoten als ein eigener Host in {CMK} überwacht wird.
Die Monitoring-Daten dieser Hosts werden aber nicht separat von Kubernetes abgerufen, sondern aus den Daten vom Kubernetes-Cluster abgezweigt.

Da Kubernetes nicht über den normalen {CMK}-Agenten abgefragt werden kann, benötigen Sie dafür den Kubernetes-xref:datasource_programs#specialagents[Spezialagenten], welcher auch als xref:datasource_programs#[Datenquellenprogramm] bezeichnet wird.
Hierbei kontaktiert {CMK} den Zielhost nicht wie üblich über den TCP Port 6556, sondern ruft stattdessen ein Hilfsprogramm auf, welches mit dem Zielsystem über die anwendungsspezifische API von Kubernetes kommuniziert.

Das Vorgehen ist wie folgt:

. Legen Sie für den Kubernetes-Master (Kubernetes Control Plane) einen Host in {CMK} an.
. Legen Sie eine Regel an, welche diesem Kubernetes-Host den Spezialagenten für Kubernetes zuordnet.

Diesen Regelsatz finden Sie unter [.guihint]#Setup > Agents > VM, Cloud, Container > Kubernetes#:

image::kubernetes_rule.png[]


Mit [.guihint]#API server endpoint# geben Sie das Ziel für Aufrufe über die Kubernetes API an.
Eine [.guihint]#Custom URL# hat z.B. die Form `\https://mykuber01.comp.lan:8443`.
Sie können aber auch über HTTP (unsicher) zugreifen.
Bei Auswahl von [.guihint]#Hostname# oder [.guihint]#IP address# wird {CMK} Namen oder IP-Adresse des von Ihnen zuvor angelegten Kubernetes-Hosts verwenden -- und als Protokoll HTTPS. Sie können dann den Standard-Port `443` verändern und einen [.guihint]#Custom path prefix# festlegen, d.h. einen Pfad, der hinten an die URL angehängt wird.
Ein Pfadpräfix ist z.B. bei xref:rancher[Rancher] wichtig, weil dort mehrere Kubernetes-Cluster aufgenommen werden können.
Die API eines einzelnen Clusters erreicht man dann z.B. unter `/k8s/cluster/mycluster`.

Unter [.guihint]#Token# geben Sie entweder das Passwort im Klartext an ([.guihint]#Explicit#) oder Sie wählen das über den Passwortspeicher aus, falls Sie es vorhin dort abgelegt haben.

Bei [.guihint]#Retrieve information about# können Sie die Kubernetes-Objekte wie Pods, Services und Deployments für die Überwachung auswählen.
Diese werden jeweils als Host abgebildet.
Wir empfehlen, dass Sie diese Hosts durch die xref:dcd#[dynamische Host-Konfiguration] automatisch verwalten lassen.

Die Bedeutung der weiteren Optionen erfahren Sie am besten aus der icon:icon_help[alt="Symbol zum Ein- oder Ausblenden der kontext-sensitiven Hilfe."] xref:user_interface#inline_help[Inline-Hilfe.]

Wenn Sie jetzt im Setup-Menü beim Kubernetes-Host die Service-Konfiguration aufrufen, sollten bereits einige der Services gefunden werden:

image::kubernetes_monitoring_cluster_services.png[]


=== Überwachung der Knoten

Damit auch die Knoten (_nodes_) überwacht werden, müssen Sie diese ebenfalls im Setup als Hosts anlegen.

Dies können Sie mit vom xref:dcd#[Dynamic Configuration Daemon (DCD)] erledigen lassen.
Oder Sie legen diese einfach von Hand als Hosts an.

Dabei ist es wichtig, dass die Host-Namen im {CMK} exakt mit den Namen der Kubernetes-Knoten übereinstimmen.
Sie können diese Namen einfach aus dem [.guihint]#Nodes#-Service des Kubernetes-Hosts ablesen.
image::kubernetes_monitoring_node_services.png[]



Übrigens: Mit dem Regelsatz [.guihint]#Setup > Agents > Access to Agents > Hostname translation for piggybacked hosts# können Sie recht flexibel Regeln definieren, nach denen Host-Namen, welche in Piggyback-Daten enthalten sind, umgewandelt werden.
Somit können Sie in {CMK} Host-Namen verwenden, welche nicht mit den Namen der Knoten übereinstimmen.

Sofern Sie auf den Knoten selbst keinen {CMK}-Agenten installiert haben, müssen Sie [.guihint]#{CMK} agent / API integrations# auf [.guihint]#No API integrations, no {CMK} agent# einstellen.


=== Labels für Kubernetes-Objekte

{CMK} erzeugt Labels für die Kubernetes-Objekte wie Knoten, Pods, Deployments oder Services während der Service-Erkennung automatisch.
Die Labels werden analog zu Docker definiert und haben die Form `cmk/kubernetes_object:OBJECT`.


== Hardware-/Software-Inventur

Die Kubernetes-Integration von {CMK} unterstützt auch die xref:inventory#[HW-/SW-Inventur.]


image::kubernetes_monitoring_hw_sw_inventory.png[]


== {CMK} entfernen

Wenn Sie den Service-Account und die Cluster-Rolle von {CMK} wieder aus Kubernetes entfernen wollen, können Sie das mit folgendem Befehl tun:

[{shell}]
----
{c-user} kubectl delete -f check_mk_rbac.yaml
namespace "check-mk" deleted
serviceaccount "check-mk" deleted
clusterrole.rbac.authorization.k8s.io "check-mk" deleted
clusterrolebinding.rbac.authorization.k8s.io "check-mk" deleted
----


[#openshift]
== Kubernetes in OpenShift-Installationen

=== Projekt anlegen

[{image-left}]
image::logo_openshift.png[width=100]

OpenShift ist eine von Red Hat entwickelte Produktreihe von Container-Anwendungsplattformen für Cloud-Computing, welche unter anderem auf Kubernetes aufbaut.

{CMK} kann auch ein OpenShift-basiertes Kubernetes überwachen.
Das Vorgehen ist sehr ähnlich zu dem oben beschriebenen,weicht aber beim Aufsetzen des Clusters für das Monitoring in einigen Details ab.

Für das Monitoring können Sie in OpenShift ein eigenes Projekt anlegen.
Das geht über die Kommandozeile mit:

[{shell-raw}]
----
{c-root} oc new-project check-mk
Now using project "check-mk" on server "https://192.168.42.62:8443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-25-centos7~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby.
----


=== Weiteres Vorgehen

Die restlichen Schritte für die Aufnahme des Clusters in das Monitoring sind wie am xref:setup[Anfang des Artikels] beschrieben.
Allerdings benutzen Sie als Kommandozeilenbefehl immer das Tool von Openshift, also `oc`, anstelle des oben beschriebenen `kubectl`. (z.B. bei der Abfrage des Service-Accounts und des Tokens).
Die IP-Adresse und den Port des Clusters können Sie sich mit folgendem Befehl ausgeben lassen:

[{shell}]
----
{c-root} oc status
----

Um den Token für den Benutzer abzurufen, benutzen Sie folgenden Befehl -- hier mit dem in diesem Artikel verwendeten Benutzer `check-mk`:

[{shell}]
----
{c-root} oc serviceaccounts get-token check-mk
----


[#rancher]
== Kubernetes in Rancher-Installationen

=== Service-Account anlegen

Mit Rancher ist die Einrichtung des Monitorings in {CMK} grundsätzlich identisch mit der xref:setup[oben] beschriebenen Variante direkt über Kubernetes.
Auch hier benötigen Sie den Service-Account, damit {CMK} auf das Cluster zugreifen kann.
Den Account erstellen Sie direkt in der Rancher-Weboberfläche, wo Sie anschließend auch dessen Token und Zertifikat finden, die Sie wiederum in {CMK} importieren.

Navigieren Sie in Rancher zunächst nach [.guihint]#Global > Security > Roles > Cluster#, um eine neue Rolle `checkmk` anzulegen:

[{image-border}]
image::rancher_roles.png[]

Der Einfachheit halber klonen Sie die Rolle [.guihint]#Cluster Owner#:

[{image-border}]
image::rancher_roles_clone.png[]

Entziehen Sie der geklonten Rolle unter [.guihint]#Grant Resources# die Rechte [.guihint]#Create,# [.guihint]#Delete,# [.guihint]#Patch# und [.guihint]#Update#:

[{image-border}]
image::rancher_roles_clone_rights.png[]

Erstellen Sie nun einen neuen Rancher-Benutzer `checkmk` unter [.guihint]#Global > Users > Add User#.
Bei [.guihint]#Global Permissions# wählen Sie die Option [.guihint]#User-Base#, um dem Benutzer nur die nötigsten Leserechte einzuräumen:

[{image-border}]
image::rancher_adduser.png[]


=== Clusterrolle zuordnen

Wechseln Sie nun zu Ihrem Cluster und klicken Sie im Cluster-Menü oben rechts auf [.guihint]#Edit.#
Hier können Sie über [.guihint]#Add Member# den eben angelegten Benutzer [.guihint]#checkmk# mit der zugehörigen Rolle [.guihint]#checkmk# zum Cluster hinzufügen:

[{image-border}]
image::rancher_addmember.png[]


=== Weiteres Vorgehen

Melden Sie sich anschließend mit dem neuen Benutzer bei Rancher an, rufen Sie den Cluster auf und klicken Sie auf [.guihint]#Kubeconfig File#.
Hier finden Sie drei Angaben, die Sie für das Monitoring in {CMK} benötigen:

* [.guihint]#clusters > cluster > server#: URL-/Pfadangaben für die xref:rule[{CMK}-Regel]
* [.guihint]#clusters > cluster > certificate-authority-data#: Das base64-kodierte Zertifikat
* [.guihint]#users > user > token#: Zugangspasswort in Form eines Bearer Tokens

image::rancher_kubeconfig.png[]

Das Zertifikat müssen Sie noch dekodieren, auf der Kommandozeile beispielsweise mit `base64 --decode` oder in einem der vielen nline-Dienste.

Die Einrichtung in {CMK} entspricht ab hier dem Vorgehen bei purer Kubernetes-Nutzung ab dem Abschnitt xref:certimport[Zertifikat in {CMK} importieren].


[#eventconsole]
== Kubernetes per Event Console überwachen

=== Rancher-Cluster aufnehmen

Wenn Sie Ihre Kubernetes-Cluster mit Rancher verwalten, können Sie die xref:ec#[Event Console] nutzen, um die Ereignisse in Rancher zu überwachen.
Die Anbindung aktivieren Sie ganz einfach für ein ganzes Cluster oder einzelne Projekte in der Rancher-Weboberfläche.

Navigieren Sie wahlweise zu Ihrem Cluster oder zu einem Projekt unter [.guihint]#Project/Namespaces# und rufen Sie dort [.guihint]#Tools > Logging# auf.
Die Konfiguration ist in beiden Fällen identisch, lediglich die Überschrift der Seite ([.guihint]#Cluster Logging# beziehungsweise [.guihint]#Project Logging#) zeigt an, wo Sie sich gerade befinden.
Wählen Sie als Ziel [.guihint]#Syslog# und tragen Sie zunächst den [.guihint]#Endpoint# ein, hier die IP-Adresse Ihres {CMK}-Servers samt Port `514`, also beispielsweise `192.168.178.101:514`. 

Das Protokoll belassen Sie bei [.guihint]#UDP.#
Unter [.guihint]#Program# tragen Sie den gewünschten Namen für den Log ein, so wie er in der Event Console erscheinen soll.
Zuletzt legen Sie unter [.guihint]#Log Severity# den Log Level fest -- zum Testen empfiehlt sich hier [.guihint]#Notice,# um auch definitiv und unmittelbar Einträge ins System zu bekommen.

[{image-border}]
image::rancher_syslog.png[]

Damit die Daten auch im Monitoring ankommen, muss in {CMK} eine entsprechende xref:ec#rules[Event Console-Regel] laufen.
In der Regeldefinition können Sie z.B. im Kasten [.guihint]#Matching Criteria# für [.guihint]#Match syslog application (tag)# den eben bei Rancher unter [.guihint]#Program# vergebenen Log-Namen eintragen, um auf diesen testweise zu filtern:

[#ec_rule]
image::kubernetes_ec_rancher_rule.png[]

[#ec_events]
Im Monitoring von {CMK} sehen Sie nun die Ereignisse Ihres Clusters oder Projekts in den Ansichten zu Events, die Sie über [.guihint]#Monitor > Event Console# und auch über das Snapin xref:user_interface#overview[Overview] erreichen:

[{image-border}]
image::rancher_syslog_events.png[]

In der Spalte [.guihint]#Application# erscheint der in der Rancher-Konfiguration unter [.guihint]#Program# festgelegte Log-Name.


=== Sonstige Cluster aufnehmen

Wenn die Cluster nicht mit einer Verwaltung wie Rancher aufgesetzt wurden, können Sie sie mittels link:https://www.fluentd.org/[Fluentd^] an die xref:ec#[Event Console] berichten lassen.
Fluentd ist eine quelloffene, universelle Logging-Lösung, die zum Beispiel für _Elasticsearch,_ aber eben auch für das Syslog-Format Daten
sammeln kann.
Sie können Fluentd sehr einfach über ein Kubernetes-DaemonSet als Container laufen lassen.

Klonen Sie zunächst das Fluentd-Repository:

[{shell}]
----
{c-user} git clone \https://github.com/fluent/fluentd-kubernetes-daemonset
----

Darin finden Sie zum einen diverse Konfigurationsdateien im YAML-Format und zum anderen die zugehörigen Docker-Dateien.
Für den Anschluss an {CMK} müssen Sie in der DaemonSet-Konfiguration `fluentd-kubernetes-daemonset/fluentd-daemonset-syslog.yaml` lediglich in Zeile 70 den Wert für `SYSLOG_HOST` setzen.

Tragen Sie als `SYSLOG_HOST` also Host-Namen oder IP-Adresse des Syslog Endpoints/{CMK}-Servers ein, etwa `192.168.178.101`.
Den `SYSLOG_PORT` belassen Sie bei `514` und das `SYSLOG_PROTOCOL` bei `udp`.
Der folgende Ausschnitt zeigt die relevanten Zeilen der Datei:

.fluentd-kubernetes-daemonset/fluentd-daemonset-syslog.yaml
[{file}]
----
---
containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-syslog
        env:
          - name:  SYSLOG_HOST
            value: "192.168.178.101"
          - name:  SYSLOG_PORT
            value: "514"
          - name:  SYSLOG_PROTOCOL
            value: "udp"
---
----

Anschließend wenden Sie das DeamonSet mit dem Kommando `kubectl` an:

[{shell}]
----
{c-user} kubectl apply -f fluentd-kubernetes-daemonset/fluentd-daemonset-syslog.yaml
----

Je nach Cluster dauert es ein wenig, bis auf jedem Knoten der Fluentd-Container läuft.

Anschließend benötigen Sie wieder eine xref:ec#rules[Event Console-Regel], die die Daten ins Monitoring bringt.
Zum Testen bietet sich hier der Wert `fluentd` als Filter für [.guihint]#Match syslog application (tag)# im Kasten [.guihint]#Matching Criteria# an, um alle Ereignisse der Fluentd-Instanzen zu bekommen.
Setzen in der Regel nun `fluentd` statt xref:ec_rule[`Rancher2`].

Sie finden das Ergebnis dann ebenso, wie xref:ec_events[oben] beschrieben im Monitoring von {CMK} in den Ansichten zu Events unter [.guihint]#Monitor > Event Console# und in der [.guihint]#Overview# -- dieses mal mit dem neuen Applikationsnamen:

[{image-border}]
image::kubernetes_ec_fluentd_events.png[]
